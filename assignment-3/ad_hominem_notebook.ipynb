{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "2e3c1e8da6e821b0193601da5a0e541c0efda9704da45b6b820f5f4c317f2def"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Load all necesary dependencies\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Files\n",
    "\n",
    "# Load both files to be able to process them\n",
    "with open('train-data-prepared.json', 'r') as f:\n",
    "  raw_train_statements = json.load(f)\n",
    "#raw_train_statements=pd.read_json('train-data-prepared.json')\n",
    "with open('val-data-prepared.json', 'r') as f:\n",
    "  raw_val_statements = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def clean_text(text: str):\n",
    "    # Parse the text using the English language model\n",
    "    # The returned object is an iterator over all tokens\n",
    "    parsed_text = nlp(text)\n",
    "    # Initialize a list which will later hold the tokens of the text\n",
    "    tokenized_clean_text = []\n",
    "    \n",
    "    # For each token in the text...\n",
    "    for token in parsed_text:\n",
    "        # If the token is _not_ one of the following, append it to\n",
    "        # the final list of tokens; continue otherwise\n",
    "        if (not token.is_punct and  # Punctuation\n",
    "                not token.is_space and  # Whitespace of any kind\n",
    "                not token.like_url and\n",
    "                not token.is_stop and \n",
    "                token.text != \">\" and \n",
    "                token.text != \"<\"\n",
    "                ): # url\n",
    "            tokenized_clean_text.append(token.text)\n",
    "    \n",
    "    # Return the list of clean tokens for this text\n",
    "    tokenized_clean_text=' '.join(tokenized_clean_text)\n",
    "    return tokenized_clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFram for Training data\n",
    "trainDF = pd.DataFrame()\n",
    "\n",
    "# Store preceding posts, ids, and labels,\n",
    "trainDF['preceding_posts'] = [statement['preceding_posts'] for statement in raw_train_statements]\n",
    "trainDF['id'] = [statement['id'] for statement in raw_train_statements]\n",
    "trainDF['label'] = [statement['label'] for statement in raw_train_statements]\n",
    "\n",
    "preceding_posts_clean = []\n",
    "\n",
    "# Iterate over post to extract body and clean text\n",
    "for statement in trainDF['preceding_posts']:\n",
    "    body_elements = []\n",
    "    for body in statement:\n",
    "        body_elements.append(clean_text(body['body']))\n",
    "    preceding_posts_clean.append(body_elements)\n",
    "\n",
    "# Create one single string for all the post in one dialog\n",
    "trainDF['preceding_posts_body_clean'] = preceding_posts_clean \n",
    "trainDF['preceding_posts_body_clean'] = trainDF['preceding_posts_body_clean'].apply(lambda x:\" \".join(x))\n",
    "\n",
    "# Store cleaned words for each post\n",
    "trainDF['preceding_posts_body_clean_sections'] = preceding_posts_clean \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFram for Validation data\n",
    "validDF = pd.DataFrame()\n",
    "\n",
    "# Store preceding posts, ids, and labels,\n",
    "validDF['preceding_posts'] = [statement['preceding_posts'] for statement in raw_val_statements]\n",
    "validDF['id'] = [statement['id'] for statement in raw_val_statements]\n",
    "validDF['label'] = [statement['label'] for statement in raw_val_statements]\n",
    "\n",
    "preceding_posts_clean = []\n",
    "\n",
    "# Iterate over post to extract body and clean text\n",
    "for statement in validDF['preceding_posts']:\n",
    "    body_elements = []\n",
    "    for body in statement:\n",
    "        body_elements.append(clean_text(body['body']))\n",
    "    preceding_posts_clean.append(body_elements)\n",
    "\n",
    "# Create one single string for all the post in one dialog\n",
    "validDF['preceding_posts_body_clean'] = preceding_posts_clean \n",
    "validDF['preceding_posts_body_clean']=validDF['preceding_posts_body_clean'].apply(lambda x:\" \".join(x))\n",
    "\n",
    "# Store cleaned words for each post\n",
    "validDF['preceding_posts_body_clean_sections'] = preceding_posts_clean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined bad words list\n",
    "bad_words_list = [\"4r5e\", \"5h1t\", \"5hit\", \"a55\", \"anal\", \"anus\", \"ar5e\", \"arrse\", \"arse\", \"ass\", \"ass-fucker\", \"asses\", \"assfucker\", \"assfukka\", \"asshole\", \"assholes\",  \"asswhole\", \"a_s_s\", \"b!tch\", \"b00bs\", \"b17ch\", \"b1tch\", \"ballbag\", \"balls\", \"ballsack\", \"bastard\", \"beastial\", \"beastiality\", \"bellend\", \"bestial\", \"bestiality\", \"bi+ch\", \"biatch\", \"bitch\", \"bitcher\", \"bitchers\", \"bitches\", \"bitchin\", \"bitching\", \"bloody\", \"blow job\", \"blowjob\", \"blowjobs\", \"boiolas\", \"bollock\", \"bollok\", \"boner\", \"boob\", \"boobs\", \"booobs\", \"boooobs\", \"booooobs\", \"booooooobs\", \"breasts\", \"buceta\", \"bugger\", \"bum\", \"bunny fucker\", \"butt\", \"butthole\", \"buttmuch\", \"buttplug\", \"c0ck\", \"c0cksucker\", \"carpet muncher\", \"cawk\", \"chink\", \"cipa\", \"cl1t\", \"clit\", \"clitoris\", \"clits\", \"cnut\", \"cock\", \"cock-sucker\", \"cockface\", \"cockhead\", \"cockmunch\", \"cockmuncher\", \"cocks\", \"cocksuck\", \"cocksucked\", \"cocksucker\", \"cocksucking\", \"cocksucks\", \"cocksuka\", \"cocksukka\", \"cok\", \"cokmuncher\", \"coksucka\", \"coon\", \"cox\", \"crap\", \"cum\", \"cummer\", \"cumming\", \"cums\", \"cumshot\", \"cunilingus\", \"cunillingus\", \"cunnilingus\", \"cunt\", \"cuntlick\", \"cuntlicker\", \"cuntlicking\", \"cunts\", \"cyalis\", \"cyberfuc\", \"cyberfuck\", \"cyberfucked\", \"cyberfucker\", \"cyberfuckers\", \"cyberfucking\", \"d1ck\", \"damn\", \"dick\", \"dickhead\", \"dildo\", \"dildos\", \"dink\", \"dinks\", \"dirsa\", \"dlck\", \"dog-fucker\", \"doggin\", \"dogging\", \"donkeyribber\", \"doosh\", \"duche\", \"dyke\", \"ejaculate\", \"ejaculated\", \"ejaculates\", \"ejaculating\", \"ejaculatings\", \"ejaculation\", \"ejakulate\", \"f u c k\", \"f u c k e r\", \"f4nny\", \"fag\", \"fagging\", \"faggitt\", \"faggot\", \"faggs\", \"fagot\", \"fagots\", \"fags\", \"fanny\", \"fannyflaps\", \"fannyfucker\", \"fanyy\", \"fatass\", \"fcuk\", \"fcuker\", \"fcuking\", \"feck\", \"fecker\", \"felching\", \"fellate\", \"fellatio\", \"fingerfuck\", \"fingerfucked\", \"fingerfucker\", \"fingerfuckers\", \"fingerfucking\", \"fingerfucks\", \"fistfuck\", \"fistfucked\", \"fistfucker\", \"fistfuckers\", \"fistfucking\", \"fistfuckings\", \"fistfucks\", \"flange\", \"fook\", \"fooker\", \"fuck\", \"fucka\", \"fucked\", \"fucker\", \"fuckers\", \"fuckhead\", \"fuckheads\", \"fuckin\", \"fucking\", \"fuckings\", \"fuckingshitmotherfucker\", \"fuckme\", \"fucks\", \"fuckwhit\", \"fuckwit\", \"fudge packer\", \"fudgepacker\", \"fuk\", \"fuker\", \"fukker\", \"fukkin\", \"fuks\", \"fukwhit\", \"fukwit\", \"fux\", \"fux0r\", \"f_u_c_k\", \"gangbang\", \"gangbanged\", \"gangbangs\", \"gaylord\", \"gaysex\", \"goatse\", \"God\", \"god-dam\", \"god-damned\", \"goddamn\", \"goddamned\", \"hardcoresex\", \"hell\", \"heshe\", \"hoar\", \"hoare\", \"hoer\", \"homo\", \"hore\", \"horniest\", \"horny\", \"hotsex\", \"jack-off\", \"jackoff\", \"jap\", \"jerk-off\", \"jism\", \"jiz\", \"jizm\", \"jizz\", \"kawk\", \"knob\", \"knobead\", \"knobed\", \"knobend\", \"knobhead\", \"knobjocky\", \"knobjokey\", \"kock\", \"kondum\", \"kondums\", \"kum\", \"kummer\", \"kumming\", \"kums\", \"kunilingus\", \"l3i+ch\", \"l3itch\", \"labia\", \"lust\", \"lusting\", \"m0f0\", \"m0fo\", \"m45terbate\", \"ma5terb8\", \"ma5terbate\", \"masochist\", \"master-bate\", \"masterb8\", \"masterbat*\", \"masterbat3\", \"masterbate\", \"masterbation\", \"masterbations\", \"masturbate\", \"mo-fo\", \"mof0\", \"mofo\", \"mothafuck\", \"mothafucka\", \"mothafuckas\", \"mothafuckaz\", \"mothafucked\", \"mothafucker\", \"mothafuckers\", \"mothafuckin\", \"mothafucking\", \"mothafuckings\", \"mothafucks\", \"mother fucker\", \"motherfuck\", \"motherfucked\", \"motherfucker\", \"motherfuckers\", \"motherfuckin\", \"motherfucking\", \"motherfuckings\", \"motherfuckka\", \"motherfucks\", \"muff\", \"mutha\", \"muthafecker\", \"muthafuckker\", \"muther\", \"mutherfucker\", \"n1gga\", \"n1gger\", \"nazi\", \"nigg3r\", \"nigg4h\", \"nigga\", \"niggah\", \"niggas\", \"niggaz\", \"nigger\", \"niggers\", \"nob\", \"nob jokey\", \"nobhead\", \"nobjocky\", \"nobjokey\", \"numbnuts\", \"nutsack\", \"orgasim\", \"orgasims\", \"orgasm\", \"orgasms\", \"p0rn\", \"pawn\", \"pecker\", \"penis\", \"penisfucker\", \"phonesex\", \"phuck\", \"phuk\", \"phuked\", \"phuking\", \"phukked\", \"phukking\", \"phuks\", \"phuq\", \"pigfucker\", \"pimpis\", \"piss\", \"pissed\", \"pisser\", \"pissers\", \"pisses\", \"pissflaps\", \"pissin\", \"pissing\", \"pissoff\", \"poop\", \"porn\", \"porno\", \"pornography\", \"pornos\", \"prick\", \"pricks\", \"pron\", \"pube\", \"pusse\", \"pussi\", \"pussies\", \"pussy\", \"pussys\", \"rectum\", \"retard\", \"rimjaw\", \"rimming\", \"s hit\", \"s.o.b.\", \"sadist\", \"schlong\", \"screwing\", \"scroat\", \"scrote\", \"scrotum\", \"semen\", \"sex\", \"sh!+\", \"sh!t\", \"sh1t\", \"shag\", \"shagger\", \"shaggin\", \"shagging\", \"shemale\", \"shi+\", \"shit\", \"shitdick\", \"shite\", \"shited\", \"shitey\", \"shitfuck\", \"shitfull\", \"shithead\", \"shiting\", \"shitings\", \"shits\", \"shitted\", \"shitter\", \"shitters\", \"shitting\", \"shittings\", \"shitty\", \"skank\", \"slut\", \"sluts\", \"smegma\", \"smut\", \"snatch\", \"son-of-a-bitch\", \"spac\", \"spunk\", \"s_h_i_t\", \"t1tt1e5\", \"t1tties\", \"teets\", \"teez\", \"testical\", \"testicle\", \"tit\", \"titfuck\", \"tits\", \"titt\", \"tittie5\", \"tittiefucker\", \"titties\", \"tittyfuck\", \"tittywank\", \"titwank\", \"tosser\", \"turd\", \"tw4t\", \"twat\", \"twathead\", \"twatty\", \"twunt\", \"twunter\", \"v14gra\", \"v1gra\", \"vagina\", \"viagra\", \"vulva\", \"w00se\", \"wang\", \"wank\", \"wanker\", \"wanky\", \"whoar\", \"whore\", \"willies\", \"willy\", \"xrated\", \"xxx\"]\n",
    "\n",
    "# This will return if the bad words are increasing among the posts\n",
    "# It will return 1 if this is the case, 0 if not.\n",
    "def bad_word_counter(group_post):\n",
    "\n",
    "    bad_words_counter = []\n",
    "    for post in group_post:\n",
    "        counter = 0\n",
    "        \n",
    "        for token in post.split():\n",
    "            if (token.lower() in bad_words_list):\n",
    "                counter = counter + 1\n",
    "        bad_words_counter.append(counter)\n",
    "    \n",
    "    # if negative increased send 1, otherwise send false\n",
    "    if (len(bad_words_counter) >= 2 ):\n",
    "        return 1 if bad_words_counter[1] > bad_words_counter[0] else 0.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# This will count upper case words for the dialog\n",
    "# Avoiding words with length <= 1 and some banned words\n",
    "def upper_counter(sentence):\n",
    "    counter = 0\n",
    "    banned_list = ['TV','EU','LA','A.','I.','OP','SF','OC']\n",
    "    for token in sentence.split():\n",
    "        if (token.isupper() and len(token) > 1 and token not in banned_list):\n",
    "            counter = counter + 1\n",
    "    return counter \n",
    "\n",
    "# This will tell if the dialog has any controversiality\n",
    "def controversiality_feature(preceding_posts) -> int:\n",
    "    found = False\n",
    "    for post_details in preceding_posts:\n",
    "        if post_details['controversiality']:\n",
    "            found = True\n",
    "    return 1 if found else 0.0\n",
    "\n",
    "# This will tell if the post has any controversiality attribute\n",
    "# This will return 1 if it has it, 0 if it does not have it.\n",
    "def violated_rule_feature(preceding_posts) -> int:\n",
    "    counter = 0\n",
    "    for post_details in preceding_posts:\n",
    "        if post_details['violated_rule'] > 0 :\n",
    "            counter = counter + post_details['violated_rule']\n",
    "    return counter\n",
    "\n",
    "# This will provide sentiment if sentiments is negative on the following post it will provide 1\n",
    "# def sentiment_feature(preceding_posts, label) -> int:\n",
    "def sentiment_feature(preceding_posts) -> int:\n",
    "    counter = 0\n",
    "    neg_records = [0 for _ in range(len(preceding_posts))]\n",
    "    index = 0\n",
    "    for post_details in preceding_posts:\n",
    "        neg_phrase_count = 0\n",
    "\n",
    "        paragraphs = post_details['body'].split('\\n')\n",
    "        for phrase in paragraphs:\n",
    "            if(len(phrase) < 1 ):\n",
    "                continue\n",
    "            phrase_blob = TextBlob(phrase)\n",
    "            \n",
    "            # for this object Sentiment(polarity=-0.6, subjectivity=1.0) we use only polarity \n",
    "            sentiment_score = phrase_blob.sentiment[0]\n",
    "            # If score is negative, count it as negative sentiment.\n",
    "            if ( sentiment_score < 0.0 ):\n",
    "                neg_phrase_count = neg_phrase_count + 1\n",
    "\n",
    "        # Save total of negative polarities on the index post\n",
    "        neg_records[index] = neg_phrase_count\n",
    "        index = index + 1\n",
    "\n",
    "    # if negative increased send 1, otherwise send false\n",
    "    if (len(neg_records) >= 2 ):\n",
    "        return 1 if neg_records[1] > neg_records[0] else 0.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURES SECCION\n",
    "\n",
    "# Tfidf\n",
    "v = TfidfVectorizer( ngram_range = (1,1), max_features=300)\n",
    "v.fit(trainDF['preceding_posts_body_clean'].values.astype('U'))\n",
    "x_train = v.transform(trainDF['preceding_posts_body_clean'].values.astype('U'))\n",
    "x_test = v.transform(validDF['preceding_posts_body_clean'].values.astype('U'))\n",
    "\n",
    "# Get Data Frame to add more features\n",
    "df_train = pd.DataFrame(x_train.toarray(),columns=v.get_feature_names())\n",
    "df_val = pd.DataFrame(x_test.toarray(),columns=v.get_feature_names())\n",
    "\n",
    "# Bad Words\n",
    "df_train['bad_words'] = trainDF['preceding_posts_body_clean_sections'].apply(bad_word_counter)\n",
    "df_val['bad_words'] = validDF['preceding_posts_body_clean_sections'].apply(bad_word_counter)\n",
    "\n",
    "# Sentiment     \n",
    "df_train['sentiment'] = trainDF['preceding_posts'].apply(sentiment_feature)\n",
    "df_val['sentiment'] = validDF['preceding_posts'].apply(sentiment_feature)\n",
    "\n",
    "# UPPER\n",
    "df_train['upper'] = trainDF['preceding_posts_body_clean'].apply(upper_counter)\n",
    "df_val['upper'] = validDF['preceding_posts_body_clean'].apply(upper_counter)\n",
    "\n",
    "# controversiality\n",
    "df_train['controversiality'] = trainDF['preceding_posts'].apply(controversiality_feature)\n",
    "df_val['controversiality'] = validDF['preceding_posts'].apply(controversiality_feature)\n",
    "\n",
    "# violated_rule\n",
    "df_train['violated_rule'] = trainDF['preceding_posts'].apply(violated_rule_feature)\n",
    "df_val['violated_rule'] = validDF['preceding_posts'].apply(violated_rule_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# Fit and predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "# fit\n",
    "clf = LinearSVC().fit(df_train, trainDF[\"label\"])\n",
    "\n",
    "# predict and save values.\n",
    "predicted = clf.predict(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A=0.6976744186046512 P=0.7073170731707317 F1=0.6904761904761904\n"
     ]
    }
   ],
   "source": [
    "# Score predictions\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "p = precision_score(validDF[\"label\"], predicted)\n",
    "f1 = f1_score(validDF[\"label\"], predicted)\n",
    "a = accuracy_score(validDF[\"label\"], predicted)\n",
    "print(\"A={} P={} F1={}\".format(a, p,f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json output\n",
    "    # Generates data.json output.\n",
    "\n",
    "output_dictionary={}\n",
    "data = validDF['id'].tolist()\n",
    "predicted_label = [str(x) for x in predicted]\n",
    "\n",
    "for id , label in zip( data , predicted_label ):\n",
    "    output_dictionary[id] = label\n",
    "\n",
    "# This will be tha output file, data.json\n",
    "with open('data.json', 'w') as json_file:\n",
    "    json.dump(output_dictionary, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}