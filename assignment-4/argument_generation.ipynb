{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "177a3d81b4b9c617590bf5eac4d5c4cc16f2d34cf46fa40a7de1b42237e71c3e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import math\r\n",
    "import operator\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "nltk.download('wordnet')\r\n",
    "Stopwords = set(stopwords.words('english'))\r\n",
    "wordlemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Farnaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Farnaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\Farnaz\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read the file to vaildate\r\n",
    "val_data = pd.read_json(r'valid_data.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Claim key words\r\n",
    "claim_key = ['should', 'tuned', 'could', 'unconstitutional', 'violate', 'might', 'violated', 'violates', 'wrong', 'rather', 'valid', 'invalid', 'irrelevant', 'inherently', 'necessarily', 'cannot', 'prevail', 'justify', 'flawed', 'merely', 'corpus', 'ought', 'inevitably', 'cause', 'justifiable', 'unacceptable', 'untrue', 'abhorrent', 'unless', 'harmful', 'punished', 'liable', 'incompatible', 'beneficial', 'justifying', 'undecided', 'skimmed', 'indefensible', 'impossible', 'undermine', 'necessary', 'flourish', 'meaningless', 'outweigh', 'substantiated', 'refute', 'jeopardized', 'incapable', 'irrational', 'heterosexual', 'immoral', 'tipped', 'preferable', 'deterred', 'wipe', 'intrinsically', 'ascending', 'deserved', 'enveloped', 'unduly', 'anyway', 'warranted', 'foreseeable', 'sure', 'justified', 'feasible', 'consummated', 'else', 'meaningful', 'fundamentally', 'falsifiable', 'inconsistent', 'brute', 'testable', 'equally', 'properly', 'actually', 'happen', 'unfounded', 'infringed', 'result', 'likely', 'safeguarded', 'irreversible', 'destroy', 'nor', 'outweighed', 'unnecessary', 'dared', 'legitimate', 'insignificant', 'unjustly', 'unprotected', 'empirically', 'acceptable', 'untenable', 'constitute', 'unrealistic', 'disappear', 'sooner', 'unlikely', 'negligible', 'retaliatory', 'needed', 'faked', 'misleading', 'contaminated', 'fail', 'socioeconomic', 'conclusive', 'undemocratic', 'deserve', 'improperly', 'inevitable', 'insult', 'breached', 'harm', 'whatsoever', 'rehired', 'safer', 'liberate', 'overboard', 'unsound', 'healthier', 'mandate', 'mutually', 'rationally', 'stifled', 'false', 'unconstitutionally', 'somehow', 'LGBTQ', 'unjust', 'ridiculous', 'really', 'otherwise', 'causal', 'legitimately', 'incorrect', 'prejudiced', 'explode', 'hers', 'discernible', 'manipulated', 'perpetrated', 'tolerated', 'lucid', 'wo', 'reliably', 'coercive', 'desirable', 'impotent', 'irresistible', 'unambiguous', 'misinterpreted', 'contradict', 'detrimental', 'reasonably', 'unchanged', 'too', 'misrepresented', 'catastrophic', 'rogue', 'ruining', 'benefit', 'manipulating', 'solve', 'correlate', 'constitutionally', 'bigotry', 'verifiable', 'obsolete', 'differently', 'happening', 'ethiopian', 'selfish', 'unethical', 'undermines', 'dangerous', 'ineffective', 'endanger', 'refuted', 'unproven', 'shall', 'dogmatic', 'ontological', 'logical', 'rightful', 'undermined', 'abuse', 'perpetuate', 'exaggerated', 'moral', 'permissible', 'ridicule', 'sincerely', 'sane', 'subdue', 'unfairly', 'inaccurate', 'discriminatory', 'erroneous', 'waste', 'distract', 'desire', 'decreases', 'insufficient', 'compelling', 'destabilize', 'sexist', 'morally', 'absurd', 'emotional', 'predicated', 'overdose', 'biologically', 'retracted', 'distort', 'adequately', 'do', 'intimidated', 'surely', 'absolutely', 'worse', 'infinite', 'terrorist', 'underdeveloped', 'ourselves', 'worth', 'materially', 'biased', 'psychologically', 'unsubstantiated', 'differentiated', 'complied', 'inappropriately', 'inseparable', 'ruin', 'constitutes', 'disclose', 'unnatural', 'poisoning', 'slitting', 'invalidated', 'contradictory', 'unlawful', 'corrupted', 'overdosing', 'contrary', 'correct', 'swallowing', 'prove', 'await', 'intolerable', 'credible', 'interfered', 'anymore', 'simply', 'militarily', 'inflict', 'disagree', 'unreliable', 'unhealthy', 'perfectly', 'forever', 'manipulate', 'inconclusive', 'contradicts', 'arouse', 'unsafe', 'despondent', 'dismisses', 'sinful', 'trafficked', 'succeed', 'deleted', 'harmless', 'trillion', 'tantamount', 'us', 'satisfactory', 'behave', 'mortal', 'copyrighted', 'harming', 'atheistic', 'wage', 'unwarranted', 'inferior', 'reasonable', 'hardly', 'fit', 'appreciate', 'suit', 'lawful', 'unjustified', 'appropriate', 'exceeded', 'futile', 'infringe', 'arbitrarily', 'grossly', 'conceal', 'amount', 'bypassing', 'decrease', 'xenophobic', 'confuse', 'darwinian', 'contradicting', 'revert', 'disrupt', 'statistically', 'desired', 'disappears', 'anabolic', 'incite', 'diminish', 'falsified', 'compatible', 'weaken', 'reduces', 'sufficient', 'mean', 'earthly', 'undesirable', 'distorted', 'miss', 'appropriately', 'inappropriate', 'satanic', 'emitted', 'flowed', 'ineligible', 'outdated', 'damned', 'proscribed', 'materialistic', 'precisely', 'affect', 'mitigating',  'solved', 'supernatural', 'entail', 'delete', 'evolve', 'trigger', 'uncomfortable', 'homosexual', 'reckless', 'lawfully', 'humane', 'cohabiting', 'rested', 'anticipate', 'eroded', 'implanted', 'excessively', 'therapeutic', 'bible', 'conveying', 'repay', 'altogether', 'oppressive', 'grounded', 'conform', 'mere', 'enslaved', 'impartial', 'collapse', 'reversing', 'unsure', 'whichever', 'sleep', 'theirs', 'outright', 'construed', 'sinister', 'artificially', 'null', 'pretend', 'compulsive', 'unscientific', 'viable', 'disparate', 'fear', 'invariably', 'individualistic', 'implying', 'correlated', 'grasp', 'prematurely', 'stupid', 'somewhere', 'discriminated', 'pretty', 'capitalist', 'accountable', 'monotheistic', 'inhumane', 'predetermined', 'compel', 'utterly', 'wiped', 'doctrinal', 'akin', 'damage', 'indirect', 'weaker', 'legitimize', 'compliant', 'misguided', 'accurate', 'innocent', 'flourishing', 'heal', 'monogamous', 'intervene', 'wrongly', 'conquer', 'doomed', 'imperialist', 'barred', 'boost', 'improves', 'subjective', 'accounting', 'virtuous', 'poisoned', 'excited', 'abnormal', 'experiential', 'verified', 'absolute', 'depend', 'wait', 'ascetic', 'insofar', 'need', 'escalate', 'eternal', 'deserves', 'modify', 'opposite', 'aggregate', 'inefficient', 'corrected', 'orderly', 'emerge', 'rendering', 'worldly', 'regretted', 'underrepresented', 'individually', 'overly', 'pluralistic', 'unreasonable', 'substantive', 'exacerbated', 'seem', 'deprive', 'irresponsible', 'yielding', 'divert', 'impact', 'decreasing', 'inherit', 'societal', 'poorer', 'stimulating', 'deter', 'delaying', 'bogus', 'confess', 'inhibit', 'overthrow', 'bipolar', 'adequate', 'interfere', 'disproportionate', 'worry', 'existent', 'indispensable', 'contacting', 'impede', 'negatively', 'imminent', 'abrupt', 'uneducated', 'rest', 'thereby', 'divine', 'coincide', 'involuntary', 'psychological', 'risk', 'compromising', 'exploitative', 'genetically', 'interstate', 'replicate', 'emerges', 'therefore', 'sic', 'mind', 'underprivileged', 'genuine', 'tailored', 'lacks', 'pornographic', 'exclude', 'apprehended', 'logically', 'incomplete', 'must', 'guaranteed', 'ignored', 'observational', 'vested', 'epistemological', 'vain', 'verify', 'impaired', 'genetic', 'curtailed', 'hanging', 'endure', 'ca', 'diminished', 'blame', 'nonviolent', 'questionable', 'bolster', 'discounted', 'reinforcing', 'leaping', 'naive', 'satisfying', 'obscene', 'subordinated', 'analogous', 'drinking', 'entails', 'inadequate', 'sufficiently', 'attributable', 'expire', 'offended', 'adversely', 'occurring', 'incredibly', 'exist', 'revitalize', 'accelerate', 'exempt', 'stems', 'neutral',\r\n",
    "    'compromised', 'expressly', 'therein', 'rational', 'complementary', 'boring', 'rhetorical', 'amounted', 'genuinely', 'burning', 'vague', 'existential', 'spying', 'intentionally', 'proletarian', 'applicable', 'marital', 'depressive', 'authentic', 'ate', 'denies', 'creed', 'objective', 'horrible', 'disruptive', 'reproduce', 'superficial', 'alive', 'productive']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Helpers\r\n",
    "\r\n",
    "# It will lematize an array of words\r\n",
    "def lemmatize_words(words):\r\n",
    "    lemmatized_words = []\r\n",
    "    for word in words:\r\n",
    "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\r\n",
    "    return lemmatized_words\r\n",
    "\r\n",
    "# Remove special characters form the string\r\n",
    "def remove_special_characters(text):\r\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\r\n",
    "    text = re.sub(regex,'',text)\r\n",
    "    return text\r\n",
    "\r\n",
    "# Clean text\r\n",
    "def get_clean_sentence(sentence):\r\n",
    "    clean_sentence = remove_special_characters(str(sentence)) \r\n",
    "    clean_sentence = re.sub(r'\\d+', '', sentence)\r\n",
    "    return clean_sentence\r\n",
    "\r\n",
    "# It returns only words matching with post tagged focus as an array\r\n",
    "def pos_tagging(text):\r\n",
    "    pos_tag = nltk.pos_tag(text)\r\n",
    "    pos_tagged_noun_verb = []\r\n",
    "    pos_tagged_focus = [\"NN\", \"NNP\", \"NNS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\r\n",
    "    for word,tag in pos_tag:\r\n",
    "        if tag in pos_tagged_focus:\r\n",
    "             pos_tagged_noun_verb.append(word)\r\n",
    "    return pos_tagged_noun_verb\r\n",
    "\r\n",
    "# Calculate Tf score\r\n",
    "def tf_score(word, sentence):\r\n",
    "    word_frequency_in_sentence = 0\r\n",
    "    len_sentence = len(sentence)\r\n",
    "    for word_in_sentence in sentence:\r\n",
    "        if word == word_in_sentence:\r\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\r\n",
    "    tf =  word_frequency_in_sentence / len_sentence\r\n",
    "    return tf\r\n",
    "\r\n",
    "# Calculates idf score\r\n",
    "def idf_score(no_of_sentences,word,sentences):\r\n",
    "    no_of_sentence_containing_word = 0\r\n",
    "    for sentence in sentences:\r\n",
    "        sentence = sentence\r\n",
    "        if word in sentence:\r\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\r\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\r\n",
    "    return idf\r\n",
    "\r\n",
    "# Calculate td_idf score by multiplying tf and idf score.\r\n",
    "def word_tfidf(word, sentence, sentences):\r\n",
    "    tf = tf_score(word, sentence)\r\n",
    "    idf = idf_score(len(sentences),word,sentences)\r\n",
    "    tf_idf = tf * idf\r\n",
    "    return tf_idf\r\n",
    "\r\n",
    "# Get tfidf score from the given sentence.\r\n",
    "def get_sentence_word_tfidf(sentence, sentences):\r\n",
    "    sentence_score = 0\r\n",
    "    pos_tagged_sentence = [] \r\n",
    "    pos_tagged_sentence = pos_tagging(sentence)\r\n",
    "    for word in pos_tagged_sentence:\r\n",
    "        sentence_score = sentence_score + word_tfidf(word, sentence, sentences)\r\n",
    "    return sentence_score\r\n",
    "\r\n",
    "# Get claim score for each sentence on the given array\r\n",
    "def get_claim_score(sentences):\r\n",
    "    row = []\r\n",
    "    for sentence in sentences:\r\n",
    "        value = 0\r\n",
    "        for word_token in word_tokenize(sentence):\r\n",
    "            if word_token in claim_key:\r\n",
    "                value += 1\r\n",
    "        row.append(value)\r\n",
    "    return row\r\n",
    "\r\n",
    "# Normalize an array\r\n",
    "def normalize_array(array):  \r\n",
    "    norm = np.linalg.norm(array)\r\n",
    "    normal_array = array/norm\r\n",
    "    return normal_array\r\n",
    "\r\n",
    "# Normalaize a 2d array by row\r\n",
    "def normalize_2d_array(multi_array):\r\n",
    "    for row in range(multi_array.shape[0]):\r\n",
    "        sum = np.sum(multi_array[row])\r\n",
    "        for col in range(multi_array.shape[1]):\r\n",
    "            multi_array[row][col] = multi_array[row][col] / sum\r\n",
    "    return multi_array\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Variable to save the summary for each argument.\r\n",
    "summaryList = []\r\n",
    "\r\n",
    "# Max number of sentences to take\r\n",
    "senteces_number = 2\r\n",
    "\r\n",
    "for text in val_data['argument']:\r\n",
    "    \r\n",
    "    # tokenized sentence\r\n",
    "    tokenized_sentences = sent_tokenize(text)\r\n",
    "    \r\n",
    "    # Clean data and save it on clean_tokenized_sentences, this will be an array of sentences(array of words)\r\n",
    "    clean_tokenized_sentences = []\r\n",
    "    for sentence in tokenized_sentences:\r\n",
    "        tokenize_group_sentences = []\r\n",
    "        clean_sentences = get_clean_sentence(sentence)\r\n",
    "        for word_token in word_tokenize(clean_sentences):\r\n",
    "            if len(word_token) > 1 and word_token not in Stopwords:\r\n",
    "                tokenize_group_sentences.append(word_token.lower())\r\n",
    "        clean_tokenized_sentences.append(lemmatize_words(tokenize_group_sentences))\r\n",
    "\r\n",
    "    sentence_with_importance = {}\r\n",
    "\r\n",
    "    # Get claim score\r\n",
    "    claim_score_array = get_claim_score(tokenized_sentences)\r\n",
    "    # Normalize claim score\r\n",
    "    claim_score_array = normalize_array(claim_score_array)\r\n",
    "\r\n",
    "    # Get TF-IDF score\r\n",
    "    sentence_word_tfidf = []\r\n",
    "    for sentence in clean_tokenized_sentences:\r\n",
    "        sentence_importance = get_sentence_word_tfidf(sentence, clean_tokenized_sentences)\r\n",
    "        sentence_word_tfidf.append(sentence_importance)\r\n",
    "\r\n",
    "    # Normalize TF-IDF score\r\n",
    "    sentence_word_tfidf = normalize_array(sentence_word_tfidf)\r\n",
    "\r\n",
    "    # Convert both scores into a 2d array and normalize it.\r\n",
    "    normalized_features = normalize_2d_array(np.vstack((sentence_word_tfidf, claim_score_array)))\r\n",
    "\r\n",
    "    # Sum both scores to get a final one.\r\n",
    "    sentence_with_importance = {}\r\n",
    "    for index in range(len(clean_tokenized_sentences)):\r\n",
    "        sentence_with_importance[index] = normalized_features[0][index] + normalized_features[1][index]\r\n",
    "\r\n",
    "    # Sort senteces based on score\r\n",
    "    sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1), reverse=True)\r\n",
    "\r\n",
    "    # Take the max number of sentences based on importance\r\n",
    "    counter = 0\r\n",
    "    summary = []\r\n",
    "    sentence_num = []\r\n",
    "    for word_prob in sentence_with_importance:\r\n",
    "        if counter < senteces_number:\r\n",
    "            sentence_num.append(word_prob[0])\r\n",
    "            counter = counter + 1\r\n",
    "        else:\r\n",
    "            break\r\n",
    "\r\n",
    "    # Sort sentences indexes to take them in order\r\n",
    "    sentence_num.sort()\r\n",
    "\r\n",
    "    # Take the selected sentences\r\n",
    "    for sentence_index in sentence_num:\r\n",
    "        summary.append(tokenized_sentences[sentence_index -1 ])\r\n",
    "    # Convert them to string\r\n",
    "    joined = \" \".join(summary)\r\n",
    "    \r\n",
    "    summaryList.append(joined)\r\n",
    "\r\n",
    "# Send all summary to the predicted DataFrame\r\n",
    "val_data['predicted'] = summaryList\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-c72a3fe5ab69>:82: RuntimeWarning: invalid value encountered in true_divide\n  normal_array = array/norm\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Generate output file predicted.json\r\n",
    "val_data.set_index('id')['predicted'].to_json('predicted.json')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}